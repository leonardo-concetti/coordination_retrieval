---
title: "Data Analysis"
author: "Leonardo Concetti"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This file contains the work-in-progress analysis of a self-paced reading experiment on the role of syntactic information in the retrieval of NP-Coordinated subjects.


# Abstract

The cue-based retrieval theory of sentence processing [3] assumes that linguistic items are encoded in Working Memory based on their featural content. Information such as syntactic category and grammatical number are encoded for each item and can then be used as cues for retrieval. Relevant questions in this respect are what exactly constitutes a single item and how detailed is the representation of hierarchical information in working memory.

  Monosyndetic NP-coordination [5] provides a unique opportunity to investigate the issue. In contrast to early syntactic analyses of coordination, many current accounts [1, 2, 4, 5] propose an asymmetric structure headed by the conjoining element. The two conjuncts are either specifier and complement in a &P [1], or attached by adjunction [4]. This analysis has direct implications for retrieval: in sentences like “The boy and the girl that the teacher praised are smiling”, the set of cues that are specified upon encountering the matrix VP “are smiling” includes [subject] and [plural]. Neither cue is however found in any of the NPs encountered in the sentence, a fact that would lead to a problematic retrieval, if there was no &P entry in WM. Under a headed account of coordination, the subject “The boy and the girl” is treated as a unique syntactic item, allowing for the [subject] and [plural] features to be encoded in WM and used as cues for retrieval, without the need for additional mechanisms.
  
  A two condition self-paced reading experiment was designed to test the hypothesis that the WM representation of a &P is distinct from the ones associated with the single NPs embedded in it. The experiment will be carried out in Italian, a language with obligatory number agreement, which also allows for the manipulation of Relative Clause attachment. The number marking on the relative clause verb allows to force a different RC attachment in the two conditions: in one case [1a] the whole &P is modified by the RC, while in the other case [1b] only the second conjunct is modified. As a consequence, activation of the &P is predicted to be boosted in [1a], while in [1b] it is the second-conjunct NP that receives the boost due to reactivation. In line with this hypothesis, longer reading times are expected at the matrix verb in [1b] compared to [1a].
  
  Sentences are presented on the screen one at a time. Every sentence is followed by a yes/no comprehension to ensure active participation.

  Data from a large sample of native speakers of Italian will be collected through Prolific (https://www.prolific.com), and analysed with Bayesian hierarchical models as well as Bayes Factor analyses. The main region of interest is the matrix verb, with additional analyses planned for the spillover region(s), as well as the RC verb.  


References
[1] Johannessen, J. B. (1998). Coordination. Oxford University Press.
[2] Kayne, R. S. (1994). The antisymmetry of syntax (Vol. 25). MIT press.
[3] Lewis, R. L., & Vasishth, S. (2005). An Activation-Based Model of Sentence Processing as Skilled Memory Retrieval. Cognitive Science, 29, 375-419.
[4] Munn, A. B. (1993). Topics in the syntax and semantics of coordinate structures. University of Maryland, College Park.
[5] Zwart, J. W. (2005). Some notes on coordination in head-final languages. Linguistics in the Netherlands, 22(1), 231-242.



Item example:

1.

a) L'assistente | e | il presidente | che | hanno sostenuto | l'intervista | con | i giornalisti | del quotidiano | *tornano* | 

velocemente | in ufficio.

  The assistant | and | the president | that | have(PL) done | the interview | with | the journalists | of the newspaper |

  *go_back(PL)* | quickly | to the office.


b) L'assistente | e | il presidente | che | ha sostenuto | l'intervista | con | i giornalisti | del quotidiano | *tornano* | 

velocemente | in ufficio.

  The assistant | and | the president | that | has(SG) done | the interview | with | the journalists | of the newspaper |

  *go_back(PL)* | quickly | to the office.  


H1: Detailed hierarchical information enter the WM representations of each item. &P is encoded in addition to the two DPs it is composed of, and is the correct target for retrieval. A slowdown is expected on the matrix verb (the critical point of retrieval) in condition 1b with respect to 1a.

H0: Detailed hierarchical information are not encoded in the WM entry of each items. There is no &P entry in Working Memory and can thus not be the target for retrieval. No difference in Reading Times is expected on the main verb in the two conditions. 




Load packages and set options

```{r Packages, include=FALSE, results='hide'}

library(MASS)
library(tidyverse)
library(extraDistr)
library(loo)
library(bridgesampling)
library(brms)
library(bayesplot)
library(lme4)
library(rstan)
library(SHELF)
library(rootSolve)
library(patchwork)
library(gt)
library(invgamma)
library(TruncatedNormal)
library(truncnorm)
library(stats)
library(reshape2)

theme_set(theme_minimal())


## Save compiled models:

rstan_options(auto_write = FALSE)

## Parallelize the chains using all the cores:

options(mc.cores = parallel::detectCores())

# To solve some conflicts between packages

select <- dplyr::select
extract <- rstan::extract


# Custom function to have a short summary of bayesian models

short_summary <- function (x, digits = 2, ...)
{
  x<- summary(x)
  cat("...\n")
    # cat(" Family: ")
    # cat(summarise_families(x$formula), "\n")
    # cat("  Links: ")
    # cat(summarise_links(x$formula, wsp = 9), "\n")
    # cat("Formula: ")
    # print(x$formula, wsp = 9)
    # cat(paste0("   Data: ", x$data_name, " (Number of observations: ",
        # x$nobs, ") \n"))
    if (!isTRUE(nzchar(x$sampler))) {
        cat("\nThe model does not contain posterior samples.\n")
    }
    else {
        final_samples <- ceiling((x$iter - x$warmup)/x$thin *
            x$chains)
        # cat(paste0("Samples: ", x$chains, " chains, each with iter = ",
        #     x$iter, "; warmup = ", x$warmup, "; thin = ", x$thin,
        #     ";\n", "         total post-warmup samples = ", final_samples,
        #     "\n\n"))
        if (nrow(x$prior)) {
            cat("Priors: \n")
            print(x$prior, show_df = FALSE)
            cat("\n")
        }
        if (length(x$splines)) {
            cat("Smooth Terms: \n")
            brms:::print_format(x$splines, digits)
            cat("\n")
        }
        if (length(x$gp)) {
            cat("Gaussian Process Terms: \n")
            brms:::print_format(x$gp, digits)
            cat("\n")
        }
        if (nrow(x$cor_pars)) {
            cat("Correlation Structures:\n")
            brms:::print_format(x$cor_pars, digits)
            cat("\n")
        }
        if (length(x$random)) {
            cat("Group-Level Effects: \n")
            for (i in seq_along(x$random)) {
                g <- names(x$random)[i]
                cat(paste0("~", g, " (Number of levels: ", x$ngrps[[g]],
                  ") \n"))
                brms:::print_format(x$random[[g]], digits)
                cat("\n")
            }
        }
        if (nrow(x$fixed)) {
            cat("Population-Level Effects: \n")
            brms:::print_format(x$fixed, digits)
            cat("\n")
        }
        if (length(x$mo)) {
            cat("Simplex Parameters: \n")
            brms:::print_format(x$mo, digits)
            cat("\n")
        }
        if (nrow(x$spec_pars)) {
            cat("Family Specific Parameters: \n")
            brms:::print_format(x$spec_pars, digits)
            cat("\n")
        }
        if (length(x$rescor_pars)) {
            cat("Residual Correlations: \n")
            brms:::print_format(x$rescor, digits)
            cat("\n")
        }
        # cat(paste0("Samples were drawn using ", x$sampler, ". "))
        if (x$algorithm == "sampling") {
            #cat(paste0("For each parameter, Bulk_ESS\n", "and Tail_ESS are effective sample size measures, ",
             #   "and Rhat is the potential\n", "scale reduction factor on split chains ",
              #  "(at convergence, Rhat = 1)."))
        }
        cat("...\n")
    }
    invisible(x)
}

```



### Data analysis

Load the dataset 

```{r}

dat <- read_csv("coord_spr.csv", show_col_types = FALSE)

```


Have a look at the RTs distribution

```{r}

hist(dat$rt)

```



```{r}

lower_bound <- quantile(dat$rt, 0.025)  
upper_bound <- quantile(dat$rt, 0.975) 

dens_dat <- ggplot(dat, aes(x = rt)) +
  geom_density(color = "black") +
  geom_ribbon(
    aes(ymin = 0, ymax = after_stat(density)), 
    alpha = 0.6, fill = "blue3", stat = "density",
    data = subset(dat, rt >= lower_bound & rt <= upper_bound)) +
  labs(x = "Reading Time",
       y = "Density") + 
  ggtitle("Reading Times with 95% interval highlighted") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12), axis.title = element_text(size = 10))

dens_dat

```

The insanely high values (mostly at the end of sentences) don't allow for a realistic visualization.


Some summary stats

```{r}

summary(dat$rt)

dat$rt[dat$rt > 100000]

dat$rt[dat$rt > 6000]

dat$rt[dat$rt < 100]

```


There are some insanely high values that are likely to be points in which the subjects took a pause (not sanctioned by the experimental procedure) during the sentence screen. We will remove all the values higher than some arbitrary threshold.


Remove RTs longer than 6000ms

```{r}

dat <- dat |> 
  filter(rt <= 6000)

```


Remove RTs shorter than 100ms, which are not meaningful data points (they are likely accidental spacebar presses that skipped an ROI before the subject could read it)

```{r}

dat <- dat |> 
  filter(rt >= 100)

```

Look at the data again


Histogram 

```{r}

hist_dat <- ggplot(
  dat,
  aes(x = rt, y = after_stat(density))
  ) +
  geom_histogram(alpha = 0.6, binwidth = 50, color = "black", fill = "blue") +
  labs(x = "Reading Time", y = "Density") +
  ggtitle("Reading Times in the whole dataset") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12), axis.title = element_text(size = 10))

hist_dat

```


Density plot

```{r}

lower_bound <- quantile(dat$rt, 0.025)  
upper_bound <- quantile(dat$rt, 0.975) 

dens_dat <- ggplot(dat, aes(x = rt)) +
  geom_density(color = "black") +
  geom_ribbon(
    aes(ymin = 0, ymax = after_stat(density)), 
    alpha = 0.6, fill = "blue3", stat = "density",
    data = subset(dat, rt >= lower_bound & rt <= upper_bound)) +
  labs(x = "Reading Time",
       y = "Density") + 
  ggtitle("Reading Times (whole dataset) with 95% interval highlighted") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12), axis.title = element_text(size = 10))

dens_dat

```



Remove low-accuracy participants (subjects 9, 71, 86, 173 --> they all have <70% accuracy on experimental items)

```{r}

dat <- dat |> 
  filter(!subj %in% c(9, 71, 86, 173))

```


We also remove subject 199, who has very weird RTs, probably due to him stopping/resuming the experiment at various points of the sentences, multiple times (half of his experimental items have insanely long RTs at various ROIs)

```{r}

dat <- dat |> 
  filter(subj != 199)

```


Sum-code the predictor

```{r}

dat <- dat |> 
  mutate(singplur = if_else(dat$condition == "sing",1/2,-1/2))

```


Generate subsets for the single regions (if necessary):


Critical region (main verb)

```{r}

mv <- dat |> 
  filter(roi_number == "10")

```


Critical -1 (pre-critical)

```{r}

mv_pre <- dat |> 
  filter(roi_number == "9")

```


Critical +1 (spillover)

```{r}

mv_spill <- dat |> 
  filter(roi_number == "11")

```


Critical +2 (end of sentence)

```{r}

end_sent <- dat |> 
  filter(roi_number == "12")

```


Relative Clause Verb

```{r}

rcv <- dat |> 
  filter(roi_number == "5")

```


RC verb -1(RC onset)

```{r}

rcv_pre <- dat |> 
  filter(roi_number == "4")

```


RC verb +1 (spillover)


```{r}

rcv_spill <- dat |> 
  filter(roi_number == "6")

```


Check latin-square design

```{r}

xtabs(~ subj + item, dat)

xtabs(~ subj + item, mv)

```



Look at the RTs distribution on the Critical Region (the Matrix Verb)

```{r}

hist_mv <- ggplot(
  mv,
  aes(x = rt, y = after_stat(density))
  ) +
  geom_histogram(alpha = 0.6, binwidth = 50, color = "black", fill = "blue") +
  labs(x = "Reading Time", y = "Density") +
  ggtitle("Reading Times at the Matrix Verb") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12), axis.title = element_text(size = 10))

hist_mv

```


```{r}

lower_bound <- quantile(mv$rt, 0.025)  
upper_bound <- quantile(mv$rt, 0.975) 

dens_dat <- ggplot(mv, aes(x = rt)) +
  geom_density(color = "black") +
  geom_ribbon(
    aes(ymin = 0, ymax = after_stat(density)), 
    alpha = 0.6, fill = "blue3", stat = "density",
    data = subset(mv, rt >= lower_bound & rt <= upper_bound)) +
  labs(x = "Reading Time",
       y = "Density") + 
  ggtitle("Reading Times (matrix verb) with 95% interval highlighted") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12), axis.title = element_text(size = 10))

dens_dat
```


Take a look at the raw RT means by condition (boxplot)

```{r}

box_mv <- ggplot(
  mv,
  aes(y = rt, x = condition)
  ) +
  geom_boxplot(alpha = 0.7) +
  labs(x = "Condition",
       y = "") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10), axis.title = element_text(size = 10))

box_mv

```
Note the big skew in the distribution, as expected in reading times.

Look at the log-transformed version

```{r}

box_logmv <- ggplot(
  mv,
  aes(y = log(rt), x = condition)
) +
  geom_boxplot(alpha = 0.7) +
  labs(x = "Condition",
       y = "") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10), axis.title = element_text(size = 10))

box_logmv

```



Look at the raw summary numbers

```{r}

mv |> 
  group_by(condition) |> 
  summarize(mean_correct = round(mean(correct), 2), 
            rt = round(mean(rt, na.rm = TRUE), ))


```

Numerically, there is no difference between the accuracy in the two conditions, but there is a difference in mean RTs at the Matrix Verb. 

**Additional visual checks**

Visualize RTs by condition for each participant

```{r, fig.height=25, fig.width=25}

rt_bysubj_scatter <- ggplot(
  mv,
  aes(y = log(rt), x = condition)
  ) +
  geom_jitter(alpha = 0.7) + 
  theme(plot.title = element_text(size = 10), axis.title = element_text(size = 10)) +
  facet_wrap(~ subj, ncol = 15)

rt_bysubj_scatter

ggsave(
  filename = "scatterplot_of_RTs_by_condition_for_each_subject.png",
  plot = rt_bysubj_scatter + theme_minimal() + theme(plot.background = element_rect(fill = "white")),
  dpi = 300,
  width = 25,  
  height = 25
  )



```


Individual boxplots

```{r, fig.height=25, fig.width=25}

rt_bysubj_box <- ggplot(
  mv,
  aes(y = log(rt), x = condition)
  ) +
  geom_boxplot(alpha = 0.7) + 
  theme(plot.title = element_text(size = 10), axis.title = element_text(size = 10)) +
  facet_wrap(~ subj, ncol = 15)

rt_bysubj_box

ggsave(
  filename = "boxplot_of_RTs_by_condition_for_each_subject.png",
  plot = rt_bysubj_box + theme_minimal() + theme(plot.background = element_rect(fill = "white")),
  dpi = 300,
  width = 25,  
  height = 25
  )


```



Take a look at individual differences in the effect in no-pooling vs hierarchical models (the no-pooling model is theoretically nonsensical here, it's just to see the difference in shrinkage)

No-pooling model

```{r,fig.height=25, fig.width=20}

fit_np <- brm(rt ~ 0 + factor(subj) + singplur:factor(subj),
              family = lognormal(),
              prior = c(
                prior(normal(0, 1), class = b),
                prior(normal(0, 1), class = sigma)),
              data = mv
              )


# parameter name of beta by subject:

ind_effects_np <- paste0("b_factorsubj",
                      unique(mv$subj), ":singplur")


beta_across_subj <- as.data.frame(fit_np) |> 
#removes the meta data from the object
  select(all_of(ind_effects_np)) |> 
  rowMeans()


# Calculate the average of these estimates
grand_av_beta <- tibble(mean = mean(beta_across_subj),
                         lq = quantile(beta_across_subj, c(0.025)),
                         hq = quantile(beta_across_subj, c(0.975)))


# make a table of beta's by subject

beta_by_subj <- posterior_summary(fit_np,
                                  variable = ind_effects_np) |> 
  as.data.frame() |> 
  mutate(subject = 1:n()) |> 
## reorder plot by magnitude of mean:
  arrange(Estimate) |> 
  mutate(subject = factor(subject, levels = subject))

ggplot(beta_by_subj,
       aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = subject)) +
  geom_point() +
  geom_errorbarh() +
  geom_vline(xintercept = grand_av_beta$mean) +
  geom_vline(xintercept = grand_av_beta$lq, linetype = "dashed") +
  geom_vline(xintercept = grand_av_beta$hq, linetype = "dashed") +
  xlab("By-subject effect of number marking on the Relative Clause verb, in log(ms)")

```





```{r, fig.height=25, fig.width=20}

fit_mv_small_indv <- brm(rt ~ singplur +
                           (singplur | subj) +
                           (singplur | item),
                         family = lognormal(),
                         prior = c(
                           prior(normal(6.5, 0.8), class = Intercept),
                           prior(normal(0, 0.02), class = b, coef = singplur),
                           prior(normal(0, 1), class = sigma),
                           prior(normal(0, 1), class = sd),
                           prior(lkj(2), class = cor)),
                         data = mv
                         )



# make a table of u_2s

ind_effects_indv <- paste0("r_subj[", unique(mv$subj),
",singplur]")

u_2_indv <- posterior_summary(fit_mv_small_indv, variable = ind_effects_indv) |> 
  as_tibble() |> 
  mutate(subj = 1:n()) |> 
  ## reorder plot by magnitude of mean:
  arrange(Estimate) |> 
  mutate(subj = factor(subj, levels = subj))

# We plot:

ggplot(u_2_indv,
       aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = subj)) +
  geom_point() +
  geom_errorbarh() +
  xlab("By-subject adjustment to the slope in log(ms)")

```

Compare the no-pooling with the hierarchical model individual estimates

```{r, fig.height=25, fig.width=20}

# Extract parameter estimates from the no pooling model:
par_np <- posterior_summary(fit_np, variable = ind_effects_np) |> 
  as_tibble() |> 
  mutate(model = "No pooling",
         subj = unique(mv$subj))

# For the hierarchical model, the code is more complicated
# because we want the effect (beta) + adjustment.

# Extract the overall group level effect:
beta <- c(as_draws_df(fit_mv_small_indv)$b_singplur)

# Extract the individual adjustments:
ind_effects_indv <- paste0("r_subj[", unique(mv$subj), ",singplur]")

adjustment <- as_draws_matrix(fit_mv_small_indv, variable = ind_effects_indv)

# Get the by subject effects in a data frame where each adjustment is in each column.

# Remove all the draws meta data by using as.data.frame
by_subj_effect <- as.data.frame(beta + adjustment)

# Summarize them by getting a table with the mean and the quantiles for each column and then binding them.
par_h <- lapply(by_subj_effect, function(x) {
  tibble(Estimate = mean(x),
         Q2.5 = quantile(x, .025),
         Q97.5 = quantile(x, .975))}) |> 
  bind_rows() |> 
  # Add a column to identify the model, and one with the subject labels:
  mutate(model = "Hierarchical",
         subj = unique(mv$subj))

# The mean and 95% CI of both models in one data frame:
by_subj_df <- bind_rows(par_h, par_np) |> 
  arrange(Estimate) |> 
  mutate(subj = factor(subj, levels = unique(.data$subj)))

b_singplur <- posterior_summary(fit_mv_small_indv, variable = "b_singplur")

ggplot(by_subj_df,
       aes(ymin = Q2.5, ymax = Q97.5, x = subj,
           y = Estimate, color = model, shape = model)) +
  geom_errorbar(position = position_dodge(1)) +
  geom_point(position = position_dodge(1)) +
  # We'll also add the mean and 95% CrI of the overall difference to the plot:
  geom_hline(yintercept = b_singplur[, "Estimate"]) +
  geom_hline(yintercept = b_singplur[, "Q2.5"],
             linetype = "dotted", linewidth = .5) +
  geom_hline(yintercept = b_singplur[, "Q97.5"],
             linetype = "dotted", linewidth = .5) +
  xlab("subj") +
  ylab("Effect of RCV number marking on MV RTs") +
  coord_flip()

```


Look at the hierarchical model individual estimates by themselves 

```{r, fig.height=25, fig.width=20}

# Extract the overall group level effect:
beta <- c(as_draws_df(fit_mv_small_indv)$b_singplur)

# Extract the individual adjustments:
ind_effects_indv <- paste0("r_subj[", unique(mv$subj), ",singplur]")

adjustment <- as_draws_matrix(fit_mv_small_indv, variable = ind_effects_indv)

# Get the by subject effects in a data frame where each adjustment is in each column.

# Remove all the draws meta data by using as.data.frame
by_subj_effect <- as.data.frame(beta + adjustment)

# Summarize them by getting a table with the mean and the quantiles for each column and then binding them.
par_h <- lapply(by_subj_effect, function(x) {
  tibble(Estimate = mean(x),
         Q2.5 = quantile(x, .025),
         Q97.5 = quantile(x, .975))}) |> 
  bind_rows() |> 
  # Add a column to identify the model, and one with the subject labels:
  mutate(model = "Hierarchical",
         subj = unique(mv$subj))

# The mean and 95% CI in a data frame:
by_subj_df <- par_h |> 
  arrange(Estimate) |> 
  mutate(subj = factor(subj, levels = unique(.data$subj)))

b_singplur <- posterior_summary(fit_mv_small_indv, variable = "b_singplur")

effect_by_subj <- ggplot(by_subj_df,
                         aes(ymin = Q2.5, ymax = Q97.5,
                             x = subj, y = Estimate)) +
  geom_errorbar(position = position_dodge(1)) +
  geom_point(position = position_dodge(1)) +
  # We'll also add the mean and 95% CrI of the overall difference to the plot:
  geom_hline(yintercept = b_singplur[, "Estimate"],
             linewidth = 0.8, color = "blue") +
  geom_hline(yintercept = b_singplur[, "Q2.5"],
             linetype = "dashed", linewidth = 0.8, color = "blue")  +
  geom_hline(yintercept = b_singplur[, "Q97.5"],
             linetype = "dashed", linewidth = 0.8, color = "blue") +
  geom_hline(yintercept = 0,
             linewidth = 0.8, color = "red") +
  xlab("subj") +
  ylab("Effect of RCV number marking on MV RTs") +
  coord_flip()


ggsave(
  filename = "Individual differences in the effect of interest.png",
  plot = effect_by_subj + theme_minimal() + theme(plot.background = element_rect(fill = "white")),
  dpi = 300,
  width = 20,  
  height = 25
  )

effect_by_subj

```




Let's proceed with a Frequentist analysis, and then the Bayesian version. 

### Frequentist LMEMs


#Critical region (Matrix Verb)

Maximal model

```{r}

mv_freq_max <- lmer(log(rt) ~ 1 + singplur + 
                      (1 + singplur | subj) + 
                      (1 + singplur | item), 
                    data = mv
                    )

summary(mv_freq_max)

```

Let's simplify the model to avoid convergence failures 



Varying Intercepts only

```{r}

mv_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                     (1 | subj) + 
                     (1 | item), 
                   data = mv
                   )

summary(mv_freq_vi)

```



Null model

```{r}

mv_freq_null <- lmer(log(rt) ~ 1 + 
                       (1 | subj) + 
                       (1 | item), 
                     data = mv
                     )

```


```{r}

anova(mv_freq_vi, mv_freq_null)

```

From a frequentist point of view, there is significant evidence in favor of our hypothesis (higher RTs in the sing condition at the Matrix Verb)



**Have a look at the residuals** 

```{r}

car::qqPlot(residuals(mv_freq_vi)) 

```


```{r}

# Add a constant to the residuals to make them all positive
residuals_pos <- residuals(mv_freq_vi) - min(residuals(mv_freq_vi)) + 1

# Create a lm object with the adjusted residuals
residuals_lm <- lm(residuals_pos ~ 1)

# Apply the Box-Cox transformation
boxcox(residuals_lm)


# Alternatively, fit a simple lm

m <- lm(rt ~ singplur, mv)

boxcox(m)

# Apply the power transformation to the response variable
mv$rt_transf <- mv$rt^-0.6

# Now, fit a linear model using the transformed response variable
m_transf <- lm(rt_transf ~ singplur, mv)

# Plot residuals

car::qqPlot(residuals(m_transf)) 

# You can then use boxcox() to confirm the transformation
bc_transf <- boxcox(m_transf)

boxcox(m_transf)


# See what happens with a lmem using this data 

mv_transformed <- lmer(rt_transf ~ 1 + singplur + 
                     (1 | subj) + 
                     (1 | item), 
                   data = mv
                   )

summary(mv_freq_vi)


```

Using this custom transform slightly improved the normality of the residuals, but there is no transform that will make them perfect at this point. Considering this and also the fact that nothing changes in terms of magnitude or significance of the fixed effect of interest, I will stick with the log-transform. 



# Spillover region (critical +1)

Varying intercepts model

```{r}

mv_spill_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                           (1 | subj) + 
                           (1 | item), 
                         data = mv_spill
                         )

summary(mv_spill_freq_vi)

```


Null model

```{r}

mv_spill_freq_null <- lmer(log(rt) ~ 1 + 
                             (1 | subj) + 
                             (1 | item), 
                           data = mv_spill
                           )

```


```{r}

anova(mv_spill_freq_vi, mv_spill_freq_null)

```

No difference here 


# Pre-critical region (critical -1)

Varying intercepts model

```{r}

mv_pre_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                         (1 | subj) + 
                         (1 | item), 
                       data = mv_pre
                       )

summary(mv_pre_freq_vi)

```


Null model

```{r}

mv_pre_freq_null <- lmer(log(rt) ~ 1 + 
                           (1 | subj) + 
                           (1 | item), 
                         data = mv_pre
                         )

```


```{r}

anova(mv_spill_freq_vi, mv_spill_freq_null)

```

No difference here 


# Relative Clause verb

Varying intercepts model

```{r}

rcv_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                      (1 | subj) + 
                      (1 | item), 
                    data = rcv
                    )

summary(rcv_freq_vi)

```


Null model

```{r}

rcv_freq_null <- lmer(log(rt) ~ 1 + 
                        (1 | subj) + 
                        (1 | item), 
                      data = rcv
                      )

```


```{r}

anova(rcv_freq_vi, rcv_freq_null)

```

Interesting, there seems to be a significant difference --> the Relative Clause verb is processed faster in the SINGULAR condition. It could be the sign of an unexpected attachment preference for the RC, where attaching it to the second conjunct is easier than attaching it to the whole &P. 

**BUT** It could also just be due to word length effects --> on the RCV the Sing condition has shorter aux ("ha" vs "hanno"). Let's see if the effect continues in its spillover region.


# Relative clause spillover region (rcv +1)

Varying intercepts model

```{r}

rcv_spill_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                            (1 | subj) + 
                            (1 | item), 
                          data = rcv_spill
                          )

summary(rcv_spill_freq_vi)

```


Null model

```{r}

rcv_spill_freq_null <- lmer(log(rt) ~ 1 + 
                              (1 | subj) + 
                              (1 | item), 
                            data = rcv_spill
                            )

```


```{r}

anova(rcv_spill_freq_vi, rcv_spill_freq_null)

```

The effect seems to be reversed in the region following the RC verb (the RC object, for instance "l'intervista / the interview" in the example sentence above), which could signal that the RCV difference was indeed due to word length difference, with subsequent compensation in the following region?


# RC onset (RC Verb -1, i.e., "che/that")

Varying intercepts model

```{r}

rcv_pre_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                          (1 | subj) + 
                          (1 | item), 
                        data = rcv_pre
                        )

summary(rcv_pre_freq_vi)

```


Null model

```{r}

rcv_pre_freq_null <- lmer(log(rt) ~ 1 + 
                            (1 | subj) + 
                            (1 | item), 
                          data = rcv_pre
                          )

```


```{r}

anova(rcv_pre_freq_vi, rcv_pre_freq_null)

```

No difference at all at the RC onset, as I would expect. 


------------------------------------------------------------------------------------------------------------------------------------------------

### Bayesian Analysis


**We run various models for each Region of Interest: 3 "agnostic" models with increasing prior sd on the slope, and in the case of the critical region, one hypothesis-driven model with a positive but small prior for the slope**


# Critical region (Matrix Verb)


Agnostic and small - N(0, 0.02)

```{r}

fit_mv_small <- brm(rt ~ singplur +
                      (singplur | subj) +
                      (singplur | item),
                    family = lognormal(),
                    prior = c(
                      prior(normal(6.5, 0.8), class = Intercept),
                      prior(normal(0, 0.02), class = b, coef = singplur),
                      prior(normal(0, 1), class = sigma),
                      prior(normal(0, 1), class = sd),
                      prior(lkj(2), class = cor)),
                    data = mv,
                    iter = 20000,
                    warmup = 2000,
                    save_pars = save_pars(all = TRUE)
                    )

#plot(fit_mv_small)

short_summary(fit_mv_small)

```


Agnostic and medium - N(0, 0.05)

```{r}

fit_mv_med <- brm(rt ~ singplur +
                    (singplur | subj) +
                    (singplur | item),
                  family = lognormal(),
                  prior = c(
                    prior(normal(6.5, 0.8), class = Intercept),
                    prior(normal(0, 0.05), class = b, coef = singplur),
                    prior(normal(0, 1), class = sigma),
                    prior(normal(0, 1), class = sd),
                    prior(lkj(2), class = cor)),
                  data = mv,
                  iter = 20000,
                  warmup = 2000,
                  save_pars = save_pars(all = TRUE)
                  )

#plot(fit_mv_med)

short_summary(fit_mv_med)

```


Agnostic and large - N(0, 1)

```{r}

fit_mv_large <- brm(rt ~ singplur +
                      (singplur | subj) +
                      (singplur | item),
                    family = lognormal(),
                    prior = c(
                      prior(normal(6.5, 0.8), class = Intercept),
                      prior(normal(0, 1), class = b, coef = singplur),
                      prior(normal(0, 1), class = sigma),
                      prior(normal(0, 1), class = sd),
                      prior(lkj(2), class = cor)),
                    data = mv,
                    iter = 20000,
                    warmup = 2000,
                    save_pars = save_pars(all = TRUE)
                    )

#plot(fit_mv_large)

short_summary(fit_mv_large)

```


Positive and small - N(0.02, 0.02)

```{r}

fit_mv_enth <- brm(rt ~ singplur +
                     (singplur | subj) +
                     (singplur | item),
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0.02, 0.02), class = b, coef = singplur),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = mv,
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

#plot(fit_mv_enth)

short_summary(fit_mv_enth)

```


Look at the posteriors for the slope

```{r}

ppd_mv_small <- mcmc_dens(fit_mv_small, pars = variables(fit_mv_small)[2]) + 
  ggtitle("Normal(0, 0.02)") +
  theme(plot.title = element_text(size = 10))

ppd_mv_med <- mcmc_dens(fit_mv_med, pars = variables(fit_mv_med)[2]) + 
  ggtitle("Normal(0, 0.05)") +
  theme(plot.title = element_text(size = 10))

ppd_mv_large <- mcmc_dens(fit_mv_large, pars = variables(fit_mv_large)[2]) + 
  ggtitle("Normal(0, 1)") +
  theme(plot.title = element_text(size = 10))

ppd_mv_enth <- mcmc_dens(fit_mv_enth, pars = variables(fit_mv_enth)[2]) + 
  ggtitle("Normal(0.02, 0.02)") +
  theme(plot.title = element_text(size = 10))

mv_post_dist <- (ppd_mv_small + ppd_mv_med) /
  (ppd_mv_large + ppd_mv_enth)

mv_post_dist


ggsave(
  filename = "mv_post_dist.png",
  plot = mv_post_dist + theme_minimal() + theme(plot.background = element_rect(fill = "white")),
  dpi = 300,
  width = 6,  
  height = 4
  )

```


Now we can look at how the different priors for the slope parameter impact the posterior difference between conditions.

Extract draws for Intercept and Slope posteriors, and compute the median difference between conditions:

```{r, results='hide', message=FALSE, warning=FALSE}

med_diff_small <- as_draws_df(fit_mv_small)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

med_diff_med <- as_draws_df(fit_mv_med)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

med_diff_large <- as_draws_df(fit_mv_large)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

med_diff_enth <- as_draws_df(fit_mv_enth)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)



```


Combine the data and generate a summary table:

```{r}

combined_med_diff_beta <- data.frame(med_diff_small, 
                                     med_diff_med, 
                                     med_diff_large, 
                                     med_diff_enth
                                     )


summ_med_diff <- data.frame(
  Prior = c("Normal(0, 0.02)", "Normal(0, 0.05)", "Normal(0, 1)", "Normal(0.02, 0.02)"),
  'Mean diff (ms)' = round(apply(combined_med_diff_beta, 2, mean), 3),
  Q2.5 = round(apply(combined_med_diff_beta, 2, function(x) quantile(x, 0.025)), 3),
  Q97.5 = round(apply(combined_med_diff_beta, 2, function(x) quantile(x, 0.975)), 3),
  check.names = FALSE
)

rownames(summ_med_diff) <- NULL


mv_post_table <- gt(summ_med_diff, rowname_col = "Prior") |> 
  tab_stubhead(label = "Prior") |> 
  tab_header("A summary, under different priors, of the posterior
distributions of the mean difference between the two conditions
reading times, on the matrix verb region,
back-transformed to the millisecond scale.") |> 
  tab_options(heading.title.font.size = 14) |> 
  opt_table_font(font = google_font("Tinos")) |> 
  tab_options(container.height = 300)


mv_post_table 

library(webshot2)

gtsave(mv_post_table, file = "mv_post_table.png")

```


Null model (varying intercepts and slopes but no predictor)

```{r}

fit_mv_null <- brm(rt ~ 1 + 
                     (singplur | subj) + 
                     (singplur | item),
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = mv,
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

short_summary(fit_mv_null)

```


**Bayes Factors**

Agnostic and small - N(0, 0.02)

```{r}

bayes_factor(fit_mv_small, fit_mv_null)

```

Agnostic and medium - N(0, 0.05)

```{r}

bayes_factor(fit_mv_med, fit_mv_null)

```


Agnostic and large - N(0, 1)

```{r}

bayes_factor(fit_mv_large, fit_mv_null)

```


Positive and small - N(0.02, 0.02)

```{r}

bayes_factor(fit_mv_enth, fit_mv_null)

```

Agnostic small vs Positive small

```{r}

bayes_factor(fit_mv_small, fit_mv_enth)

```


# Spillover region (critical +1)


# Pre-critical region (critical -1)



# Relative Clause Verb


Agnostic and small - N(0, 0.02)

```{r}

fit_rcv_small <- brm(rt ~ singplur +
                       (singplur | subj) +
                       (singplur | item),
                     family = lognormal(),
                     prior = c(
                       prior(normal(6.5, 0.8), class = Intercept),
                       prior(normal(0, 0.02), class = b, coef = singplur),
                       prior(normal(0, 1), class = sigma),
                       prior(normal(0, 1), class = sd),
                       prior(lkj(2), class = cor)),
                     data = rcv,
                     iter = 20000,
                     warmup = 2000,
                     save_pars = save_pars(all = TRUE)
                     )

#plot(fit_rcv_small)

short_summary(fit_rcv_small)

```


Agnostic and medium - N(0, 0.05)

```{r}

fit_rcv_med <- brm(rt ~ singplur +
                     (singplur | subj) +
                     (singplur | item),
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0, 0.05), class = b, coef = singplur),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = rcv,
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

#plot(fit_rcv_med)

short_summary(fit_rcv_med)

```


Agnostic and large - N(0, 1)

```{r}

fit_rcv_large <- brm(rt ~ singplur +
                       (singplur | subj) +
                       (singplur | item),
                     family = lognormal(),
                     prior = c(
                       prior(normal(6.5, 0.8), class = Intercept),
                       prior(normal(0, 1), class = b, coef = singplur),
                       prior(normal(0, 1), class = sigma),
                       prior(normal(0, 1), class = sd),
                       prior(lkj(2), class = cor)),
                     data = rcv,
                     iter = 20000,
                     warmup = 2000,
                     save_pars = save_pars(all = TRUE)
                     )

#plot(fit_rcv_large)

short_summary(fit_rcv_large)

```



Negative and medium (because of word length) (-0.04, 0.05)


```{r}

fit_rcv_neg <- brm(rt ~ singplur +
                     (singplur | subj) +
                     (singplur | item),
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(-0.04, 0.05), class = b, coef = singplur),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = rcv,
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

```





Model with word length difference as a predictor 

```{r}

rcv_word <- dat |> 
  mutate(aux_lenght = ifelse(condition == "plur", 5, 2))

fit_rcv_word <- brm(rt ~ singplur + wordl +
                      (singplur + wordl | subj) +
                      (singplur + wordl | item),
                    family = lognormal(),
                    prior = c(
                      prior(normal(6.5, 0.8), class = Intercept),
                      prior(normal(0, 0.05), class = b, coef = singplur),
                      prior(normal(0, 0.05), class = b, coef = wordl),
                      prior(normal(0, 1), class = sigma),
                      prior(normal(0, 1), class = sd),
                      prior(lkj(2), class = cor)),
                    data = rcv_word,
                    iter = 20000,
                    warmup = 2000,
                    save_pars = save_pars(all = TRUE)
                    )

```





Look at the posteriors for the slope

```{r}

ppd_rcv_small <- mcmc_dens(fit_rcv_small, pars = variables(fit_rcv_small)[2]) + 
  ggtitle("Normal(0, 0.02)") +
  theme(plot.title = element_text(size = 10))

ppd_rcv_med <- mcmc_dens(fit_rcv_med, pars = variables(fit_rcv_med)[2]) + 
  ggtitle("Normal(0, 0.05)") +
  theme(plot.title = element_text(size = 10))

ppd_rcv_large <- mcmc_dens(fit_rcv_large, pars = variables(fit_rcv_large)[2]) + 
  ggtitle("Normal(0, 1)") +
  theme(plot.title = element_text(size = 10))

ppd_rcv_enth_post <- mcmc_dens(fit_rcv_enth_post, pars = variables(fit_rcv_enth_post)[2]) + 
  ggtitle("Normal(0.06, 0.02)") +
  theme(plot.title = element_text(size = 10))

rcv_post_dist <- (ppd_rcv_small_post + ppd_rcv_med_post) /
  (ppd_rcv_large_post + ppd_rcv_enth_post)

rcv_post_dist

ggsave("rcv_post_dist.png", rcv_post_dist, width = 6, height = 4, dpi = 300)

```


Now we can look at how the different priors for the slope parameter impact the posterior difference between conditions.

Extract draws for Intercept and Slope posteriors, and compute the median difference between conditions:

```{r, results='hide', message=FALSE, warning=FALSE}

rcv_med_diff_small <- as_draws_df(fit_rcv_small_post)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

rcv_med_diff_med <- as_draws_df(fit_rcv_med_post)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

rcv_med_diff_large <- as_draws_df(fit_rcv_large_post)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

rcv_med_diff_enth <- as_draws_df(fit_rcv_enth_post)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)



```


Combine the data and generate a summary table:

```{r}

combined_rcv_med_diff_beta <- data.frame(rcv_med_diff_small, 
                                         rcv_med_diff_med, 
                                         rcv_med_diff_large, 
                                         rcv_med_diff_enth
                                         )


rcv_summ_med_diff <- data.frame(
  Prior = c("Normal(0, 0.02)", "Normal(0, 0.05)", "Normal(0, 1)", "Normal(0.06, 0.02)"),
  'Mean diff (ms)' = round(apply(combined_rcv_med_diff_beta, 2, mean), 3),
  Q2.5 = round(apply(combined_rcv_med_diff_beta, 2, function(x) quantile(x, 0.025)), 3),
  Q97.5 = round(apply(combined_rcv_med_diff_beta, 2, function(x) quantile(x, 0.975)), 3),
  check.names = FALSE
)

rownames(rcv_summ_med_diff) <- NULL


rcv_post_table <- gt(rcv_summ_med_diff, rowname_col = "Prior") |> 
  tab_stubhead(label = "Prior") |> 
  tab_header("A summary, under different priors, of the posterior
distributions of the mean difference between the two conditions
reading times, on the matrix verb region,
back-transformed to the millisecond scale.") |> 
  tab_options(heading.title.font.size = 14) |> 
  opt_table_font(font = google_font("Tinos")) |> 
  tab_options(container.height = 300)


rcv_post_table

gtsave(rcv_post_table, file = "rcv_post_table.png")

```


Null model (varying intercepts but no predictor)

```{r}

fit_rcv_null <- brm(rt ~ 1 + 
                     (singplur | subj) + 
                     (singplur | item),
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = rcv,
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

short_summary(fit_rcv_null)

```


**Bayes Factors**

Agnostic and small - N(0, 0.02)

```{r}

bayes_factor(fit_rcv_small_post, fit_rcv_null)


```

Agnostic and medium - N(0, 0.05)

```{r}

bayes_factor(fit_rcv_med_post, fit_rcv_null)

```


Agnostic and large - N(0, 1)

```{r}

bayes_factor(fit_rcv_large_post, fit_rcv_null)

```


Positive and small - N(0.06, 0.02)

```{r}

bayes_factor(fit_rcv_enth_post, fit_rcv_null)

```




# Relative Clause Verb spillover region (rcv +1)


Agnostic and small - N(0, 0.02)

```{r}

fit_rcv_spill_small_post <- brm(rt ~ singplur +
                           (singplur | subj) +
                           (singplur | item),
                         family = lognormal(),
                         prior = c(
                           prior(normal(6.5, 0.8), class = Intercept),
                           prior(normal(0, 0.02), class = b, coef = singplur),
                           prior(normal(0, 1), class = sigma),
                           prior(normal(0, 1), class = sd),
                           prior(lkj(2), class = cor)),
                         data = rcv_spill,
                         #iter = 20000,
                         #warmup = 2000,
                         #save_pars = save_pars(all = TRUE)
                         )

#plot(fit_rcv_small_post)

short_summary(fit_rcv_small_post)

```


Agnostic and medium - N(0, 0.05)

```{r}

fit_rcv_spill_med_post <- brm(rt ~ singplur +
                         (singplur | subj) +
                         (singplur | item),
                       family = lognormal(),
                       prior = c(
                         prior(normal(6.5, 0.8), class = Intercept),
                         prior(normal(0, 0.05), class = b, coef = singplur),
                         prior(normal(0, 1), class = sigma),
                         prior(normal(0, 1), class = sd),
                         prior(lkj(2), class = cor)),
                       data = rcv_spill,
                       #iter = 20000,
                       #warmup = 2000,
                       #save_pars = save_pars(all = TRUE)
                       )

#plot(fit_rcv_med_post)

short_summary(fit_rcv_med_post)

```


Agnostic and large - N(0, 1)

```{r}

fit_rcv_spill_large_post <- brm(rt ~ singplur +
                           (singplur | subj) +
                           (singplur | item),
                         family = lognormal(),
                         prior = c(
                           prior(normal(6.5, 0.8), class = Intercept),
                           prior(normal(0, 1), class = b, coef = singplur),
                           prior(normal(0, 1), class = sigma),
                           prior(normal(0, 1), class = sd),
                           prior(lkj(2), class = cor)),
                         data = rcv_spill,
                         #iter = 20000,
                         #warmup = 2000,
                         #save_pars = save_pars(all = TRUE)
                         )

#plot(fit_rcv_large_post)

short_summary(fit_rcv_large_post)

```


Positive and small - N(0.06, 0.02)

```{r}

fit_rcv_spill_enth_post <- brm(rt ~ singplur +
                          (singplur | subj) +
                          (singplur | item),
                        family = lognormal(),
                        prior = c(
                          prior(normal(6.5, 0.8), class = Intercept),
                          prior(normal(0.06, 0.02), class = b, coef = singplur),
                          prior(normal(0, 1), class = sigma),
                          prior(normal(0, 1), class = sd),
                          prior(lkj(2), class = cor)),
                        data = rcv_spill,
                        #iter = 20000,
                        #warmup = 2000,
                        #save_pars = save_pars(all = TRUE)
                        )

#plot(fit_rcv_enth_post)

short_summary(fit_rcv_enth_post)

```


Look at the posteriors for the slope

```{r}

ppd_rcv_spill_small_post <- mcmc_dens(fit_rcv_spill_small_post, pars = variables(fit_rcv_spill_small_post)[2]) + 
  ggtitle("Normal(0, 0.02)") +
  theme(plot.title = element_text(size = 10))

ppd_rcv_spill_med_post <- mcmc_dens(fit_rcv_spill_med_post, pars = variables(fit_rcv_spill_med_post)[2]) + 
  ggtitle("Normal(0, 0.05)") +
  theme(plot.title = element_text(size = 10))

ppd_rcv_spill_large_post <- mcmc_dens(fit_rcv_spill_large_post, pars = variables(fit_rcv_spill_large_post)[2]) + 
  ggtitle("Normal(0, 1)") +
  theme(plot.title = element_text(size = 10))

ppd_rcv_spill_enth_post <- mcmc_dens(fit_rcv_spill_enth_post, pars = variables(fit_rcv_spill_enth_post)[2]) + 
  ggtitle("Normal(0.06, 0.02)") +
  theme(plot.title = element_text(size = 10))

rcv_spill_post_dist <- (ppd_rcv_spill_small_post + ppd_rcv_spill_med_post) /
  (ppd_rcv_spill_large_post + ppd_rcv_spill_enth_post)

rcv_spill_post_dist

ggsave("rcv_spill_post_dist.png", rcv_spill_post_dist, width = 6, height = 4, dpi = 300)

```


Now we can look at how the different priors for the slope parameter impact the posterior difference between conditions.

Extract draws for Intercept and Slope posteriors, and compute the median difference between conditions:

```{r, results='hide', message=FALSE, warning=FALSE}

rcv_spill_med_diff_small <- as_draws_df(fit_rcv_spill_small_post)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

rcv_spill_med_diff_med <- as_draws_df(fit_rcv_spill_med_post)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

rcv_spill_med_diff_large <- as_draws_df(fit_rcv_spill_large_post)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

rcv_spill_med_diff_enth <- as_draws_df(fit_rcv_spill_enth_post)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)



```


Combine the data and generate a summary table:

```{r}

combined_rcv_spill_med_diff_beta <- data.frame(rcv_spill_med_diff_small, 
                                         rcv_spill_med_diff_med, 
                                         rcv_spill_med_diff_large, 
                                         rcv_spill_med_diff_enth
                                         )


rcv_spill_summ_med_diff <- data.frame(
  Prior = c("Normal(0, 0.02)", "Normal(0, 0.05)", "Normal(0, 1)", "Normal(0.06, 0.02)"),
  'Mean diff (ms)' = round(apply(combined_rcv_spill_med_diff_beta, 2, mean), 3),
  Q2.5 = round(apply(combined_rcv_spill_med_diff_beta, 2, function(x) quantile(x, 0.025)), 3),
  Q97.5 = round(apply(combined_rcv_spill_med_diff_beta, 2, function(x) quantile(x, 0.975)), 3),
  check.names = FALSE
)

rownames(rcv_spill_summ_med_diff) <- NULL


rcv_spill_post_table <- gt(rcv_spill_summ_med_diff, rowname_col = "Prior") |> 
  tab_stubhead(label = "Prior") |> 
  tab_header("A summary, under different priors, of the posterior
distributions of the mean difference between the two conditions
reading times, on the matrix verb region,
back-transformed to the millisecond scale.") |> 
  tab_options(heading.title.font.size = 14) |> 
  opt_table_font(font = google_font("Tinos")) |> 
  tab_options(container.height = 300)


rcv_spill_post_table

gtsave(rcv_spill_post_table, file = "rcv_post_table.png")

```


Null model (varying intercepts but no predictor)

```{r}

fit_rcv_spill_null <- brm(rt ~ 1 + 
                     (singplur | subj) + 
                     (singplur | item),
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = rcv_spill,
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

short_summary(fit_rcv_spill_null)

```


**Bayes Factors**

Agnostic and small - N(0, 0.02)

```{r}

bayes_factor(fit_rcv_spill_small_post, fit_rcv_spill_null)



```

Agnostic and medium - N(0, 0.05)

```{r}

bayes_factor(fit_rcv_spill_med_post, fit_rcv_spill_null)

```


Agnostic and large - N(0, 1)

```{r}

bayes_factor(fit_rcv_spill_large_post, fit_rcv_spill_null)

```


Positive and small - N(0.06, 0.02)

```{r}

bayes_factor(fit_rcv_spill_enth_post, fit_rcv_spill_null)

```



# Relative Clause onset region (rcv -1)


---------------------------------------------------------------------------------------------------------------------------------------------

### Slower subjects 

Considering the positive correlation (.63) between the by-subject sd on the intercepts and slopes, it could be the case that the effect is much stronger in "slower" subjects, which could be the ones that really took the time to deeply process the sentences. Let's see what happens if we subset the df only keeping the slower (for example mean rt > 400-500 ms) subjects.


```{r}

dat_slow <- dat |> 
  group_by(subj) |> 
  filter(mean(rt) > 500)

```


```{r}

length(unique(dat_slow$subj))

```

This leaves us with 125/240 subjects


Look at the numbers

```{r}

dat_slow |> 
  filter(roi_number == 10) |> 
  group_by(condition) |> 
  summarize(mean_correct = round(mean(correct), 2), 
            rt = round(mean(rt, na.rm = TRUE), ))


```

The raw numerical effect size is a bit higher than the one for the complete dataset. Let's look at a boxplot

```{r}

box_logmv <- ggplot(
  data = subset(dat_slow, roi_number == 10),
  aes(y = log(rt), x = condition)
) +
  geom_boxplot(alpha = 0.7) +
  labs(x = "Condition",
       y = "") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10), axis.title = element_text(size = 10))

box_logmv

```

Run a frequentist LMEM


Varying Intercepts only

```{r}

mv_freq_vi_slow <- lmer(log(rt) ~ 1 + singplur + 
                     (1 | subj) + 
                     (1 | item), 
                   subset(dat_slow, roi_number == 10)
                   )

summary(mv_freq_vi_slow)

```

Run a Bayesian version

```{r}

fit_mv_slow <- brm(rt ~ 1 + singplur +
                     (singplur | subj) +
                     (singplur | item), 
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0, 0.02), class = b, coef = singplur),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = subset(dat_slow, roi_number == 10),
                   iter = 4000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

short_summary(fit_mv_slow)

```


```{r}

fit_mv_slow_null <- brm(rt ~ 1 + 
                     (singplur | subj) +
                     (singplur | item), 
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd)),
                   data = subset(dat_slow, roi_number == 10),
                   iter = 4000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )


short_summary(fit_mv_slow_null)

```


```{r}

bayes_factor(fit_mv_slow, fit_mv_slow_null)

```


The effect is bigger and BF gives stronger evidence in its favor. 

