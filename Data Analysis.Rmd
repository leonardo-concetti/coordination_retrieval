---
title: "Data Analysis"
author: "Leonardo Concetti"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This file contains the analysis of a self-paced reading experiment on the role of syntactic reactivation in the retrieval of DP-Coordination subjects.


# Abstract

The goal of the present study is to empirically investigate the reality of an understudied component of the cue-based retrieval model (Lewis & Vasishth, 2005): activation boost due to successive retrievals. This is achieved by leveraging the hierarchical configuration of DP-coordination: Relative Clause attachment is manipulated in a 2-condition design, such that different memory representations are reactivated in the two conditions. 

Method: 240 native speakers of Italian, recruited through Prolific, completed a moving window self-paced reading task. 2-condition design; 32 experimental items; 64 filler items; yes/no comprehension questions after each trial.

Item example:


(a) L'assistente | e | il presidente | che | hanno sostenuto | l'intervista | con | i giornalisti | del quotidiano | *tornano* | 

velocemente | in ufficio.

  The assistant | and | the president | that | have(PL) done | the interview | with | the journalists | of the newspaper |

  *go_back(PL)* | quickly | to the office.


b) L'assistente | e | il presidente | che | ha sostenuto | l'intervista | con | i giornalisti | del quotidiano | *tornano* | 

velocemente | in ufficio.

  The assistant | and | the president | that | has(SG) done | the interview | with | the journalists | of the newspaper |

  *go_back(PL)* | quickly | to the office.  


In both conditions, retrieval of the full coordination ("the assistant and the president") is necessary at the matrix verb (“return”). Manipulating the number marking on the Relative Clause (RC) verb (singular vs plural) forces different RC attachments in the two conditions: the whole coordination is retrieved and interpreted as the subject of the RC in (a), while only the second conjunct (the president) is retrieved in (b). If reactivation boosts activation, retrieval at the matrix verb should be facilitated more in (a) than in (b), leading to faster reading times. Therefore, we predict higher reading times at the matrix verb in condition (b) with respect to (a).





Load packages and set options

```{r Packages, include=FALSE, results='hide'}

library(tidyverse)
library(bridgesampling)
library(brms)
library(lme4)
library(rstan)
library(patchwork)
library(gt)
library(ggtext)


theme_set(theme_minimal())


## Save compiled models:

rstan_options(auto_write = FALSE)

## Parallelize the chains using all the cores:

options(mc.cores = parallel::detectCores())

# To solve some conflicts between packages

select <- dplyr::select
extract <- rstan::extract


# Custom function to have a short summary of bayesian models

short_summary <- function (x, digits = 2, ...)
{
  x<- summary(x)
  cat("...\n")
    # cat(" Family: ")
    # cat(summarise_families(x$formula), "\n")
    # cat("  Links: ")
    # cat(summarise_links(x$formula, wsp = 9), "\n")
    # cat("Formula: ")
    # print(x$formula, wsp = 9)
    # cat(paste0("   Data: ", x$data_name, " (Number of observations: ",
        # x$nobs, ") \n"))
    if (!isTRUE(nzchar(x$sampler))) {
        cat("\nThe model does not contain posterior samples.\n")
    }
    else {
        final_samples <- ceiling((x$iter - x$warmup)/x$thin *
            x$chains)
        # cat(paste0("Samples: ", x$chains, " chains, each with iter = ",
        #     x$iter, "; warmup = ", x$warmup, "; thin = ", x$thin,
        #     ";\n", "         total post-warmup samples = ", final_samples,
        #     "\n\n"))
        if (nrow(x$prior)) {
            cat("Priors: \n")
            print(x$prior, show_df = FALSE)
            cat("\n")
        }
        if (length(x$splines)) {
            cat("Smooth Terms: \n")
            brms:::print_format(x$splines, digits)
            cat("\n")
        }
        if (length(x$gp)) {
            cat("Gaussian Process Terms: \n")
            brms:::print_format(x$gp, digits)
            cat("\n")
        }
        if (nrow(x$cor_pars)) {
            cat("Correlation Structures:\n")
            brms:::print_format(x$cor_pars, digits)
            cat("\n")
        }
        if (length(x$random)) {
            cat("Group-Level Effects: \n")
            for (i in seq_along(x$random)) {
                g <- names(x$random)[i]
                cat(paste0("~", g, " (Number of levels: ", x$ngrps[[g]],
                  ") \n"))
                brms:::print_format(x$random[[g]], digits)
                cat("\n")
            }
        }
        if (nrow(x$fixed)) {
            cat("Population-Level Effects: \n")
            brms:::print_format(x$fixed, digits)
            cat("\n")
        }
        if (length(x$mo)) {
            cat("Simplex Parameters: \n")
            brms:::print_format(x$mo, digits)
            cat("\n")
        }
        if (nrow(x$spec_pars)) {
            cat("Family Specific Parameters: \n")
            brms:::print_format(x$spec_pars, digits)
            cat("\n")
        }
        if (length(x$rescor_pars)) {
            cat("Residual Correlations: \n")
            brms:::print_format(x$rescor, digits)
            cat("\n")
        }
        # cat(paste0("Samples were drawn using ", x$sampler, ". "))
        if (x$algorithm == "sampling") {
            #cat(paste0("For each parameter, Bulk_ESS\n", "and Tail_ESS are effective sample size measures, ",
             #   "and Rhat is the potential\n", "scale reduction factor on split chains ",
              #  "(at convergence, Rhat = 1)."))
        }
        cat("...\n")
    }
    invisible(x)
}

```


### Data pre-processing


Load the complete dataset

```{r}

dat <- read_csv("coord_spr_tot.csv", show_col_types = FALSE)

```


Check which subject have <70% accuracy on experimental items

```{r}

dat |> 
  group_by(subj) |> 
  summarize(mean_correct_exp = round(mean(correct[condition != "fill"]), 2)) |> 
  filter(mean_correct_exp < 0.7)

```


Check for very long RTs

```{r}

dat |> 
  group_by(subj) |> 
  filter(condition != "fill") |> 
  summarize(mean_correct = round(mean(correct), 2),
            rt_rc_pre = round(mean(rt[roi_number == 4], na.rm = TRUE), ),
            rt_rc = round(mean(rt[roi_number == 5], na.rm = TRUE), ),
            rt_rc_spill = round(mean(rt[roi_number == 6], na.rm = TRUE), ),
            rt_mv_pre = round(mean(rt[roi_number == 9], na.rm = TRUE), ),
            rt_mv = round(mean(rt[roi_number == 10], na.rm = TRUE), ),
            rt_mv_spill = round(mean(rt[roi_number == 11], na.rm = TRUE), ),
            rt_end = round(mean(rt[roi_number == 12], na.rm = TRUE), ),
            rt_tot = round(mean(rt, na.rm = TRUE), )
            ) |> 
  filter(rt_tot > 2000)

```


Remove the fillers

```{r}

dat <- dat |> 
  filter(condition != "fill")

```


Remove RTs longer than 6000ms, which are likely to reflect unsanctioned pauses, rather than cognitive processes of interest.

```{r}

dat <- dat |> 
  filter(rt <= 6000)

```


Remove RTs shorter than 100ms, which are not meaningful data points (they are likely accidental spacebar presses that skipped an ROI before the subject could read it)

```{r}

dat <- dat |> 
  filter(rt >= 100)

```


Remove low-accuracy participants (subjects 9, 71, 86, 173 --> they all have <70% accuracy on experimental items)

```{r}

dat <- dat |> 
  filter(!subj %in% c(9, 71, 86, 173))

```


Also remove subject 199, who has very high RTs, probably due to him stopping/resuming the experiment at various points of the sentences, multiple times (half of his experimental items have insanely long RTs at various ROIs)

```{r}

dat |> filter(subj == 199, rt>2000, roi_number == 10) |> pull(rt)

dat <- dat |> 
  filter(subj != 199)

```


### Data analysis


Sum-code the predictor

```{r}

dat <- dat |> 
  mutate(singplur = if_else(dat$condition == "sing",1/2,-1/2))

```


Generate subsets for the single regions:


Critical region (main verb)

```{r}

mv <- dat |> 
  filter(roi_number == "10")

```


Critical -1 (pre-critical)

```{r}

mv_pre <- dat |> 
  filter(roi_number == "9")

```


Critical +1 (spillover)

```{r}

mv_spill <- dat |> 
  filter(roi_number == "11")

```


Critical +2 (end of sentence)

```{r}

end_sent <- dat |> 
  filter(roi_number == "12")

```


Relative Clause Verb

```{r}

rcv <- dat |> 
  filter(roi_number == "5")

```


RC verb -1(RC onset)

```{r}

rcv_pre <- dat |> 
  filter(roi_number == "4")

```


RC verb +1 (spillover)


```{r}

rcv_spill <- dat |> 
  filter(roi_number == "6")

```


Look at the raw summary numbers

```{r}

mv |> 
  group_by(condition) |> 
  summarize(mean_correct = round(mean(correct), 2), 
            RT = round(mean(rt, na.rm = TRUE), ))

```

Numerically, there is no difference between the accuracy in the two conditions, but there is a difference in mean RTs at the Matrix Verb. 


Plot raw RTs by condition by region

```{r, fig.height=8, fig.width=20}

# Calculate mean RTs and SE by condition and by ROI

summary_data <- dat %>%
  group_by(condition, roi_number) %>%
  summarise(
    mean_rt = mean(rt),
    se_rt = sd(rt) / sqrt(n())
  )



# Define x-axis labels with HTML <br> tags for new lines and bold for the critical region
x_labels <- c(
  "L'assistente<br><br>The assistant", 
  "e<br><br>and", 
  "il presidente<br><br>the president", 
  "che<br><br>that", 
  "ha/hanno sostenuto<br><br>has/have done", 
  "l'intervista<br><br>the interview", 
  "con<br><br>with", 
  "i giornalisti<br><br>the journalists", 
  "del quotidiano<br><br>of the newspaper", 
  "***tornano***<br><br>***return***",  # Bold the critical region "tornano"
  "velocemente<br><br>quickly", 
  "in ufficio.<br><br>to the office."
)

# Create a named vector for the labels
names(x_labels) <- 1:12

# Convert the named vector to a factor with levels
summary_data$roi_number <- factor(summary_data$roi_number, levels = 1:12)

rt_bycond_byroi <- ggplot(summary_data, aes(x = roi_number, y = mean_rt, shape = condition, group = condition, linetype = condition)) +
  geom_line(linewidth = 0.8) + 
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = mean_rt - se_rt, ymax = mean_rt + se_rt), width = 0.2) +
  scale_shape_manual(values = c("sing" = 2, "plur" = 0)) +  # Assign hollow triangle to "sing" and hollow square to "plur"
  scale_x_discrete(labels = x_labels) +
  labs(
    title = "Reading Time by Region",
    x = "Region of Interest",
    y = "Mean Reading Time (ms)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 20, face = "bold", margin = margin(b = 20)),
    axis.title.x = element_text(size = 18, margin = margin(t = 20)),
    axis.title.y = element_text(size = 18, margin = margin(r = 20)),
    axis.text.x = element_markdown(size = 15, margin = margin(t = 5)),  # Use element_markdown to support markdown
    axis.text.y = element_text(size = 14),
    legend.title = element_text(size = 20),
    legend.text = element_text(size = 18),
    plot.margin = margin(t = 10, l = 10, r = 10, b = 10)  # Adjust margins if necessary
  )
  

rt_bycond_byroi

```


Now the same but on log-transformed RTs

```{r, fig.height=8, fig.width=20}

# Calculate log mean RTs and SE by condition and by ROI

log_summary_data <- dat %>%
  group_by(condition, roi_number) %>%
  summarise(
    mean_rt = mean(log(rt)),
    se_rt = sd(log(rt)) / sqrt(n())
  )


# Define x-axis labels with HTML <br> tags for new lines and bold for the critical region
x_labels <- c(
  "L'assistente<br><br>The assistant", 
  "e<br><br>and", 
  "il presidente<br><br>the president", 
  "che<br><br>that", 
  "ha/hanno sostenuto<br><br>has/have done", 
  "l'intervista<br><br>the interview", 
  "con<br><br>with", 
  "i giornalisti<br><br>the journalists", 
  "del quotidiano<br><br>of the newspaper", 
  "***tornano***<br><br>***return***",  # Bold the critical region "tornano"
  "velocemente<br><br>quickly", 
  "in ufficio.<br><br>to the office."
)

# Create a named vector for the labels
names(x_labels) <- 1:12

# Convert the named vector to a factor with levels
log_summary_data$roi_number <- factor(log_summary_data$roi_number, levels = 1:12)

log_rt_bycond_byroi <- ggplot(log_summary_data, aes(x = roi_number, y = mean_rt, shape = condition, group = condition, linetype = condition)) +
  geom_line(linewidth = 0.8) + 
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = mean_rt - se_rt, ymax = mean_rt + se_rt), width = 0.2) +
  scale_shape_manual(values = c("sing" = 2, "plur" = 0)) +  # Assign hollow triangle to "sing" and hollow square to "plur"
  scale_x_discrete(labels = x_labels) +
  labs(
    title = "Reading Time by Region, log-transformed",
    x = "Region of Interest",
    y = "Mean Reading Time (log(ms))"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 20, face = "bold", margin = margin(b = 20)),
    axis.title.x = element_text(size = 18, margin = margin(t = 20)),
    axis.title.y = element_text(size = 18, margin = margin(r = 20)),
    axis.text.x = element_markdown(size = 15, margin = margin(t = 5)),  # Use element_markdown to support markdown
    axis.text.y = element_text(size = 14),
    legend.title = element_text(size = 20),
    legend.text = element_text(size = 18),
    plot.margin = margin(t = 10, l = 10, r = 10, b = 10)  # Adjust margins if necessary
  )
  

log_rt_bycond_byroi

```


### Frequentist LMEMs


# Critical region (Matrix Verb)

Maximal model

```{r}

mv_freq_max <- lmer(log(rt) ~ 1 + singplur + 
                      (1 + singplur | subj) + 
                      (1 + singplur | item), 
                    data = mv
                    )

summary(mv_freq_max)

```


Simplify the model to avoid convergence failures 


Varying Intercepts only

```{r}

mv_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                     (1 | subj) + 
                     (1 | item), 
                   data = mv
                   )

summary(mv_freq_vi)

```


Null model

```{r}

mv_freq_null <- lmer(log(rt) ~ 1 + 
                       (1 | subj) + 
                       (1 | item), 
                     data = mv
                     )

```


ANOVA likelihood ratio test

```{r}

anova(mv_freq_vi, mv_freq_null)

```

From a frequentist point of view, we can reject the null hypothesis (i.e., that the difference between conditions is 0)


# Spillover region (critical +1)


Varying intercepts model

```{r}

mv_spill_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                           (1 | subj) + 
                           (1 | item), 
                         data = mv_spill
                         )

summary(mv_spill_freq_vi)

```


Null model

```{r}

mv_spill_freq_null <- lmer(log(rt) ~ 1 + 
                             (1 | subj) + 
                             (1 | item), 
                           data = mv_spill
                           )

```


ANOVA likelihood ratio test

```{r}

anova(mv_spill_freq_vi, mv_spill_freq_null)

```

No difference here 


# Pre-critical region (critical -1)

Varying intercepts model

```{r}

mv_pre_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                         (1 | subj) + 
                         (1 | item), 
                       data = mv_pre
                       )

summary(mv_pre_freq_vi)

```


Null model

```{r}

mv_pre_freq_null <- lmer(log(rt) ~ 1 + 
                           (1 | subj) + 
                           (1 | item), 
                         data = mv_pre
                         )

```


ANOVA likelihood ratio test

```{r}

anova(mv_spill_freq_vi, mv_spill_freq_null)

```

No difference between condition at the pre-critical region.


# Check if the effect on the critical region survives after accounting for the RTs of the pre-critical region

Autoregressive(1) model:

Generate a column for RT-1

```{r}

dat <- dat |> 
  group_by(subj, item) |>             # Lag within each trial
  mutate(rt_lag = lag(rt)) |>         # Create lagged RT
  ungroup()

dat <- dat |> mutate(log_rt_lag = log(rt_lag))

dat <- dat |> mutate(c_log_rt_lag = log_rt_lag - mean(log_rt_lag, na.rm = TRUE))

mv <- dat |> 
  filter(roi_number == "10")

```


Model

```{r}

mv_ar1 <- lmer(log(rt) ~ singplur + c_log_rt_lag +
                 (1 | subj) + (1 | item),
               data = mv)

summary(mv_ar1)

```

The AR(1) model, taken together with the non-significant effect on the pre-critical region, confirms that the effect on the critical region is likely due to the experimental manipulation, and not to spillover from the preceding region.


Alternative check: model with position as a fixed effect

```{r}

dat <- dat |> 
  mutate(
    roi_number = factor(roi_number, levels = sort(unique(roi_number)))
  )

contrasts(dat$roi_number) <- contr.helmert(length(unique(dat$roi_number)))

model_pos_helmert <- lmer(log(rt) ~ condition * roi_number + 
                            (1 | subj) + 
                            (1 | item),
                          data = dat)

summary(model_pos_helmert)


```

The model output (specifically the interaction term "conditionsing:roi_number9") shows that the difference between conditions at the critical region (roi_number = 10) is significantly different from the difference across all previous regions.

All 3 pieces (pre-critical non significant, AR(1) and cond*pos models) point in the direction that the effect we observe at the critical region is actually due to the experimental manipulation and not to spillover from previous regions.


# Relative Clause verb

Varying intercepts model

```{r}

rcv_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                      (1 | subj) + 
                      (1 | item), 
                    data = rcv
                    )

summary(rcv_freq_vi)

```


Null model

```{r}

rcv_freq_null <- lmer(log(rt) ~ 1 + 
                        (1 | subj) + 
                        (1 | item), 
                      data = rcv
                      )

```

ANOVA likelihood ratio test

```{r}

anova(rcv_freq_vi, rcv_freq_null)

```

The model shows significant evidence against the null hypothesis on the Relative Clause verb --> the RC verb is processed faster in the SINGULAR condition. Since Italian tends to show a preference for High Attachment in complex DPs (like PP-chains or DP-coordination), this difference is likely to be due to word length effects --> on the RCV the SING condition has shorter auxiliary ("ha" vs "hanno"). 


Include centered word length (wl, expressed as number of characters) as fixed effect

Center word length

```{r}

dat <- dat |> 
  mutate(c_wl = wl - mean(wl))

rcv <- dat |> 
  filter(roi_number == "5")

```


Model with centered wl as fixed effect

```{r}

rcv_freq_vi_c_wl <- lmer(log(rt) ~ 1 + singplur + c_wl +
                      (1 | subj) + 
                      (1 | item), 
                    data = rcv
                    )

summary(rcv_freq_vi_c_wl)

```


Based on this model, it looks like the effect of condition (singplur) goes in the opposite (and expected) direction when we take word length into account: the singular condition is estimated to elicit numerically longer RTs than the plural condition. The effect is non significant in frequentist terms (t = 1.2). Word length, on the other hand, is estimated to lead to longer RTs significantly


Residualization method

```{r}

fit_wl <- lm(log(rt) ~ c_wl, data = rcv)

rcv <- rcv |> 
  mutate(residual_rt = residuals(fit_wl))

```


Model with residual RTs as dependent variable

```{r}

rcv_freq_vi_res <- lmer(residual_rt ~ singplur +
                               (1 + singplur | subj) +
                               (1 + singplur | item),
                             data = rcv)

summary(rcv_freq_vi_res)

```


To me it looks like the model with c_wl in the fixed effects shows that the difference between conditions is mainly due to word length differences, with a small, non-significant effect of condition, such that SING is read slower than PLUR The residualization method seem to suggest that after accounting for word length differences, the SING condition is read significantly slower than PLUR. Both models go in the same direction, but the first method would not give a "significant" effect of condition, while the second one does. In both cases, once we account for word length differences, the difference between conditions is not longer negative (i.e., SING faster than PLUR), but positive (SING slower than PLUR), which is compatible with the fact that Italian native speaker should be slightly biased towards a preference for High Attachment of the relative clause, which likely leads to a slowdown in the "SING" condition (which is the Low Attachment condition) due to a disconfermed prediction. 


# Relative clause spillover region (rcv +1)

Varying intercepts model

```{r}

rcv_spill_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                            (1 | subj) + 
                            (1 | item), 
                          data = rcv_spill
                          )

summary(rcv_spill_freq_vi)

```


Null model

```{r}

rcv_spill_freq_null <- lmer(log(rt) ~ 1 + 
                              (1 | subj) + 
                              (1 | item), 
                            data = rcv_spill
                            )

```


```{r}

anova(rcv_spill_freq_vi, rcv_spill_freq_null)

```


In the spillover region (the RC object, for instance "l'intervista / the interview" in the example sentence above), where word length is no longer different, the effect clearly persists, with the singular condition eliciting slower reading times than the plural condition. 


# RC onset (RC Verb -1, i.e., "che/that")

Varying intercepts model

```{r}

rcv_pre_freq_vi <- lmer(log(rt) ~ 1 + singplur + 
                          (1 | subj) + 
                          (1 | item), 
                        data = rcv_pre
                        )

summary(rcv_pre_freq_vi)

```


Null model

```{r}

rcv_pre_freq_null <- lmer(log(rt) ~ 1 + 
                            (1 | subj) + 
                            (1 | item), 
                          data = rcv_pre
                          )

```


```{r}

anova(rcv_pre_freq_vi, rcv_pre_freq_null)

```

No difference at the RC onset.


------------------------------------------------------------------------------------------------------------------------------------------------

### Bayesian Analysis


**We run various models for each Region of Interest: 3 "agnostic" models (the models encode the a-priori assumption that the effect will be centered around 0, with varying degrees of uncertainty). For the critical region, one hypothesis-driven model with a positive but small prior for the slope is also defined. This model encodes the a-priori assumption that the difference between conditions will be positive, centered around 25ms, with possible values ranging from 0 to 150ms**


# Critical region (Matrix Verb)


Agnostic and small - N(0, 0.02)

```{r}

fit_mv_small <- brm(rt ~ singplur +
                      (singplur | subj) +
                      (singplur | item),
                    family = lognormal(),
                    prior = c(
                      prior(normal(6.5, 0.8), class = Intercept),
                      prior(normal(0, 0.02), class = b, coef = singplur),
                      prior(normal(0, 1), class = sigma),
                      prior(normal(0, 1), class = sd),
                      prior(lkj(2), class = cor)),
                    data = mv,
                    iter = 20000,
                    warmup = 2000,
                    save_pars = save_pars(all = TRUE)
                    )


short_summary(fit_mv_small)

```


Agnostic and medium - N(0, 0.05)

```{r}

fit_mv_med <- brm(rt ~ singplur +
                    (singplur | subj) +
                    (singplur | item),
                  family = lognormal(),
                  prior = c(
                    prior(normal(6.5, 0.8), class = Intercept),
                    prior(normal(0, 0.05), class = b, coef = singplur),
                    prior(normal(0, 1), class = sigma),
                    prior(normal(0, 1), class = sd),
                    prior(lkj(2), class = cor)),
                  data = mv,
                  iter = 20000,
                  warmup = 2000,
                  save_pars = save_pars(all = TRUE)
                  )

short_summary(fit_mv_med)

```


Agnostic and large - N(0, 0.1)

```{r}

fit_mv_large <- brm(rt ~ singplur +
                      (singplur | subj) +
                      (singplur | item),
                    family = lognormal(),
                    prior = c(
                      prior(normal(6.5, 0.8), class = Intercept),
                      prior(normal(0, 0.1), class = b, coef = singplur),
                      prior(normal(0, 1), class = sigma),
                      prior(normal(0, 1), class = sd),
                      prior(lkj(2), class = cor)),
                    data = mv,
                    iter = 20000,
                    warmup = 2000,
                    save_pars = save_pars(all = TRUE)
                    )

short_summary(fit_mv_large)

```


Positive and small - N(0.04, 0.02)

```{r}

fit_mv_enth <- brm(rt ~ singplur +
                     (singplur | subj) +
                     (singplur | item),
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0.04, 0.02), class = b, coef = singplur),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = mv,
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

short_summary(fit_mv_enth)

```


Look at the posteriors for the slope

```{r}

ppd_mv_small <- mcmc_dens(fit_mv_small, pars = variables(fit_mv_small)[2]) + 
  ggtitle("Normal(0, 0.02)") +
  theme(plot.title = element_text(size = 10))

ppd_mv_med <- mcmc_dens(fit_mv_med, pars = variables(fit_mv_med)[2]) + 
  ggtitle("Normal(0, 0.05)") +
  theme(plot.title = element_text(size = 10))

ppd_mv_large <- mcmc_dens(fit_mv_large, pars = variables(fit_mv_large)[2]) + 
  ggtitle("Normal(0, 1)") +
  theme(plot.title = element_text(size = 10))

ppd_mv_enth <- mcmc_dens(fit_mv_enth, pars = variables(fit_mv_enth)[2]) + 
  ggtitle("Normal(0.04, 0.02)") +
  theme(plot.title = element_text(size = 10))

mv_post_dist <- (ppd_mv_small + ppd_mv_med) /
  (ppd_mv_large + ppd_mv_enth)

mv_post_dist

ggsave(
  filename = "mv_post_dist.png",
  plot = mv_post_dist + theme_minimal() + theme(plot.background = element_rect(fill = "white")),
  dpi = 300,
  width = 6,  
  height = 4
  )

```


Look at how the different priors for the slope parameter impact the posterior difference between conditions.

Extract draws for Intercept and Slope posteriors, and compute the median difference between conditions:

```{r, results='hide', message=FALSE, warning=FALSE}

med_diff_small <- as_draws_df(fit_mv_small)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

med_diff_med <- as_draws_df(fit_mv_med)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

med_diff_large <- as_draws_df(fit_mv_large)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)

med_diff_enth <- as_draws_df(fit_mv_enth)[c("b_singplur", "b_Intercept")] |> 
  mutate(
    med_diff = (exp(b_Intercept + b_singplur) - exp(b_Intercept - b_singplur))
  ) |> 
  select(med_diff)



```


Combine the data and generate a summary table:

```{r}

combined_med_diff_beta <- data.frame(med_diff_small, 
                                     med_diff_med,
                                     med_diff_large,
                                     med_diff_enth
                                     )


summ_med_diff <- data.frame(
  Prior = c("Normal(0, 0.02)", "Normal(0, 0.05)", "Normal(0, 0.1)", "Normal(0.04, 0.02)"),
  'Mean diff (ms)' = round(apply(combined_med_diff_beta, 2, mean), ),
  Q2.5 = round(apply(combined_med_diff_beta, 2, function(x) quantile(x, 0.025)), ),
  Q97.5 = round(apply(combined_med_diff_beta, 2, function(x) quantile(x, 0.975)), ),
  check.names = FALSE
)

rownames(summ_med_diff) <- NULL


mv_post_table <- gt(summ_med_diff, rowname_col = "Prior") |> 
  tab_stubhead(label = "Prior") |> 
  #tab_header("A summary, under different priors, of the posterior
#distributions of the mean difference between the two conditions
#reading times, on the matrix verb region,
#back-transformed to the millisecond scale.") |> 
  tab_options(heading.title.font.size = 14) |> 
  opt_table_font(font = google_font("Tinos")) |> 
  tab_options(container.height = 300)


mv_post_table 

library(webshot2)

gtsave(mv_post_table, file = "mv_post_table3.png")

```


Null model (varying intercepts and slopes but no predictor)

```{r}

fit_mv_null <- brm(rt ~ 1 + 
                     (singplur | subj) + 
                     (singplur | item),
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = mv,
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

short_summary(fit_mv_null)

```


### Bayes Factors

Agnostic and small - N(0, 0.02)

```{r}

B10 <- bayes_factor(fit_mv_small, fit_mv_null)

B10

```

Agnostic and medium - N(0, 0.05)

```{r}

B20 <- bayes_factor(fit_mv_med, fit_mv_null)

B20

```


Agnostic and large - N(0, 0.1)

```{r}

B30 <- bayes_factor(fit_mv_large, fit_mv_null)

B30

```


Positive and small - N(0.04, 0.02)

```{r}

B40 <- bayes_factor(fit_mv_enth, fit_mv_null)

B40

```


Combine the BFs into a tibble

```{r}

bayes_factors <- tibble(
  Name = c("BF<sub>10</sub> - N(0, 0.1)", "BF<sub>20</sub> - N(0, 0.05)",  "BF<sub>30</sub> - N(0, 0.02)", "BF<sub>40</sub> - N(0.04, 0.02)"),
  `Bayes Factor` = c(3, 6, 8, 12)
)

```


Make it into a table

```{r}


BF_table <- bayes_factors |> 
  gt() |> 
  cols_label(
    Name = "Comparison",
    `Bayes Factor` = "Bayes Factor"
  ) |> 
  tab_style(
    style = cell_text(weight = "normal"),
    locations = cells_column_labels(everything())
  ) |> 
  fmt_number(
    columns = c(`Bayes Factor`),
    decimals = 0
  ) |> 
  opt_table_font(font = google_font("Tinos"))|> 
  tab_options(heading.title.font.size = 14) |> 
  fmt_markdown(columns = c(Name))

BF_table

gtsave(BF_table, file = "bf_table4.png")

```


# Relative Clause Verb

Models with word length as fixed effect:


Agnostic and small - N(0, 0.02)

```{r}

fit_rcv_small <- brm(rt ~ singplur + c_wl +
                       (singplur | subj) +
                       (singplur | item),
                     family = lognormal(),
                     prior = c(
                       prior(normal(6.5, 0.8), class = Intercept),
                       prior(normal(0, 0.02), class = b, coef = singplur),
                       prior(normal(0, 0.05), class = b, coef = c_wl),
                       prior(normal(0, 1), class = sigma),
                       prior(normal(0, 1), class = sd),
                       prior(lkj(2), class = cor)),
                     data = rcv,
                     iter = 20000,
                     warmup = 2000,
                     save_pars = save_pars(all = TRUE)
                     )

short_summary(fit_rcv_small)

```


Agnostic and medium - N(0, 0.05)

```{r}

fit_rcv_med <- brm(rt ~ singplur + c_wl +
                     (singplur | subj) +
                     (singplur | item),
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0, 0.05), class = b, coef = singplur),
                     prior(normal(0, 0.05), class = b, coef = c_wl),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = rcv,
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

short_summary(fit_rcv_med)

```


Look at the posteriors for the slope

```{r}

ppd_rcv_small <- mcmc_dens(fit_rcv_small, pars = variables(fit_rcv_small)[2]) + 
  ggtitle("Normal(0, 0.02)") +
  theme(plot.title = element_text(size = 10))

ppd_rcv_med <- mcmc_dens(fit_rcv_med, pars = variables(fit_rcv_med)[2]) + 
  ggtitle("Normal(0, 0.05)") +
  theme(plot.title = element_text(size = 10))




rcv_post_dist <- (ppd_rcv_small + ppd_rcv_med)

rcv_post_dist

```


Models using residual RTs as dependent variable:


Agnostic and small - N(0, 0.02)

```{r}

fit_rcv_small_res <- brm(residual_rt ~ singplur +
                       (singplur | subj) +
                       (singplur | item),
                     family = gaussian(),
                     prior = c(
                       prior(normal(6.5, 0.8), class = Intercept),
                       prior(normal(0, 0.02), class = b, coef = singplur),
                       prior(normal(0, 1), class = sigma),
                       prior(normal(0, 1), class = sd),
                       prior(lkj(2), class = cor)),
                     data = rcv,
                     iter = 20000,
                     warmup = 2000,
                     save_pars = save_pars(all = TRUE)
                     )

short_summary(fit_rcv_small_res)

```


Agnostic and medium - N(0, 0.05)

```{r}

fit_rcv_med_res <- brm(residual_rt ~ singplur +
                     (singplur | subj) +
                     (singplur | item),
                   family = gaussian(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0, 0.05), class = b, coef = singplur),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = rcv,
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

short_summary(fit_rcv_med_res)

```


---------------------------------------------------------------------------------------------------------------------------------------------

### Slower subjects 

Considering the positive correlation (.63) between the by-subject sd on the intercepts and slopes, it could be the case that the effect is much stronger in "slower" subjects, which could be the ones that really took the time to deeply process the sentences. Let's see what happens if we subset the df only keeping the slower (for example mean rt > 400-500 ms) subjects.


```{r}

dat_slow <- dat |> 
  group_by(subj) |> 
  filter(mean(rt) > 500)

```


```{r}

length(unique(dat_slow$subj))

length(unique(dat$subj))


```

This leaves us with 123/235 subjects


Look at the numbers

```{r}

dat_slow |> 
  filter(roi_number == 10) |> 
  group_by(condition) |> 
  summarize(mean_correct = round(mean(correct), 2), 
            rt = round(mean(rt, na.rm = TRUE), ))


```

The raw numerical effect size is a bit higher than the one for the complete dataset (but the two values are obviously also higher).


Run a frequentist LMEM


Varying Intercepts only

```{r}

mv_freq_vi_slow <- lmer(log(rt) ~ 1 + singplur + 
                     (1 | subj) + 
                     (1 | item), 
                   subset(dat_slow, roi_number == 10)
                   )

summary(mv_freq_vi_slow)

```

Run a Bayesian version

```{r}

fit_mv_slow <- brm(rt ~ 1 + singplur +
                     (singplur | subj) +
                     (singplur | item), 
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0, 0.02), class = b, coef = singplur),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd),
                     prior(lkj(2), class = cor)),
                   data = subset(dat_slow, roi_number == 10),
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )

short_summary(fit_mv_slow)

```


Null model

```{r}

fit_mv_slow_null <- brm(rt ~ 1 + 
                     (singplur | subj) +
                     (singplur | item), 
                   family = lognormal(),
                   prior = c(
                     prior(normal(6.5, 0.8), class = Intercept),
                     prior(normal(0, 1), class = sigma),
                     prior(normal(0, 1), class = sd)),
                   data = subset(dat_slow, roi_number == 10),
                   iter = 20000,
                   warmup = 2000,
                   save_pars = save_pars(all = TRUE)
                   )


short_summary(fit_mv_slow_null)

```


Bayes factor

```{r}

bayes_factor(fit_mv_slow, fit_mv_slow_null)

```


The effect is bigger and BF gives stronger evidence in its favor. 

